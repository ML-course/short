{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from preamble import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-on Machine Learning with Python (sklearn)\n",
    "Joaquin Vanschoren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# scikit-learn\n",
    "One of the most prominent Python libraries for machine learning:\n",
    "\n",
    "* Contains many state-of-the-art machine learning algorithms\n",
    "* Builds on numpy (fast), implements advanced techniques\n",
    "* Wide range of evaluation measures and techniques\n",
    "* Offers [comprehensive documentation](http://scikit-learn.org/stable/documentation) about each algorithm\n",
    "* Widely used, and a wealth of [tutorials](http://scikit-learn.org/stable/user_guide.html) and code snippets are available \n",
    "* Works well with numpy, scipy, pandas, matplotlib,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Algorithms\n",
    "See the [Reference](http://scikit-learn.org/dev/modules/classes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Supervised learning:__\n",
    "\n",
    "* Linear models (Ridge, Lasso, Elastic Net, ...)\n",
    "* Support Vector Machines\n",
    "* Tree-based methods (Classification/Regression Trees, Random Forests,...)\n",
    "* Nearest neighbors\n",
    "* Neural networks \n",
    "* Gaussian Processes\n",
    "* Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Unsupervised learning:__\n",
    "    \n",
    "* Clustering (KMeans, ...)\n",
    "* Matrix Decomposition (PCA, ...)\n",
    "* Manifold Learning (Embeddings)\n",
    "* Density estimation\n",
    "* Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Model selection and evaluation:__\n",
    "\n",
    "* Cross-validation\n",
    "* Grid-search\n",
    "* Lots of metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data import\n",
    "Multiple options:\n",
    "\n",
    "* A few toy datasets are included in `sklearn.datasets`\n",
    "* Import [1000s of datasets](http://www.openml.org) via `sklearn.datasets.fetch_openml`\n",
    "* You can import data files (CSV) with `pandas` or `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, fetch_openml\n",
    "iris_data = load_iris()\n",
    "dating_data = fetch_openml(\"SpeedDating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These will return a `Bunch` object (similar to a `dict`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of iris_dataset: dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, pre\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys of iris_dataset: {}\".format(iris_data.keys()))\n",
    "print(iris_data['DESCR'][:193] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Targets (classes) and features are lists of strings\n",
    "* Data and target values are always numeric (ndarrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: ['setosa' 'versicolor' 'virginica']\n",
      "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Shape of data: (150, 4)\n",
      "First 5 rows:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "Targets:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Targets: {}\".format(iris_data['target_names']))\n",
    "print(\"Features: {}\".format(iris_data['feature_names']))\n",
    "print(\"Shape of data: {}\".format(iris_data['data'].shape))\n",
    "print(\"First 5 rows:\\n{}\".format(iris_data['data'][:5]))\n",
    "print(\"Targets:\\n{}\".format(iris_data['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building models\n",
    "All scikitlearn _estimators_ follow the same interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class SupervisedEstimator(...):\n",
    "    def __init__(self, hyperparam, ...):\n",
    "\n",
    "    def fit(self, X, y):   # Fit/model the training data\n",
    "        ...                # given data X and targets y\n",
    "        return self\n",
    "     \n",
    "    def predict(self, X):  # Make predictions\n",
    "        ...                # on unseen data X  \n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y): # Predict and compare to true\n",
    "        ...                # labels y                \n",
    "        return score\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training and testing data\n",
    "To evaluate our classifier, we need to test it on unseen data.  \n",
    "`train_test_split`: splits data randomly in 75% training and 25% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (112, 4)\n",
      "y_train shape: (112,)\n",
      "X_test shape: (38, 4)\n",
      "y_test shape: (38,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris_data['data'], iris_data['target'], \n",
    "    random_state=0)\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also choose other ways to split the data. For instance, the following will create a training set of 10% of the data and a test set of 5% of the data. This is useful when dealing with very large datasets. `stratify` defines the target feature to stratify the data (ensure that the class distributions are kept the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xs_train shape: (15, 4)\n",
      "Xs_test shape: (8, 4)\n"
     ]
    }
   ],
   "source": [
    "X, y = iris_data['data'], iris_data['target']\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(X,y, stratify=y, train_size=0.1, test_size=0.05)\n",
    "print(\"Xs_train shape: {}\".format(Xs_train.shape))\n",
    "print(\"Xs_test shape: {}\".format(Xs_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitting a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we'll build is a k-Nearest Neighbor classifier.  \n",
    "kNN is included in `sklearn.neighbors`, so let's build our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Making predictions\n",
    "Let's create a new example and ask the kNN model to classify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0]\n",
      "Predicted target name: ['setosa']\n"
     ]
    }
   ],
   "source": [
    "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "prediction = knn.predict(X_new)\n",
    "print(\"Prediction: {}\".format(prediction))\n",
    "print(\"Predicted target name: {}\".format(\n",
    "       iris_data['target_names'][prediction]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating the model\n",
    "Feeding all test examples to the model yields all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set predictions:\n",
      " [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "print(\"Test set predictions:\\n {}\".format(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `score` function computes the percentage of correct predictions\n",
    "\n",
    "``` python\n",
    "knn.score(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "print(\"Score: {:.2f}\".format(knn.score(X_test, y_test) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a single train-test split, we can use `cross_validate` do run a cross-validation. \n",
    "It will return the test scores, as well as the fit and score times, for every fold.\n",
    "By default, scikit-learn does a 5-fold cross-validation, hence returning 5 test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.001, 0.001, 0.001, 0.001, 0.001]),\n",
       " 'score_time': array([0.002, 0.002, 0.002, 0.002, 0.002]),\n",
       " 'test_score': array([0.967, 0.967, 0.933, 0.933, 1.   ]),\n",
       " 'train_score': array([1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "xval = cross_validate(knn, X, y, return_train_score=True, n_jobs=-1)\n",
    "xval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean should give a better performance estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(xval['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introspecting the model\n",
    "Most models allow you to retrieve the trained model parameters, usually called `coef_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.153, -0.025,  0.267,  0.574])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching these with the names of the features, we can see which features are primarily used by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('petal length (cm)', 0.2669801292888398),\n",
       " ('petal width (cm)', 0.5738618608875328),\n",
       " ('sepal length (cm)', -0.15330145645467907),\n",
       " ('sepal width (cm)', -0.02540761074550385)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = zip(iris_data.feature_names,lr.coef_)\n",
    "set(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation procedures\n",
    "### Holdout\n",
    "The simplest procedure is [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), which splits arrays or matrices into random train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create a synthetic dataset\n",
    "X, y = make_blobs(centers=2, random_state=0)\n",
    "# split data and labels into a training and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "# Instantiate a model and fit it to the training set\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "# evaluate the model on the test set\n",
    "print(\"Test set score: {:.2f}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-validation\n",
    "- [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html?highlight=cross%20val%20score#sklearn.model_selection.cross_val_score)\n",
    "    - `cv` parameter defines the kind of cross-validation splits, default is 5-fold CV\n",
    "    - `scoring` defines the scoring metric. Also see below.\n",
    "    - Returns list of all scores. Models are built internally, but not returned\n",
    "- [cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html?highlight=cross%20validate#sklearn.model_selection.cross_validate)\n",
    "    - Similar, but also returns the fit and test times, and allows multiple scoring metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.967 1.    0.933 0.967 1.   ]\n",
      "Average cross-validation score: 0.97\n",
      "Variance in cross-validation score: 0.0006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iris = load_iris()\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(logreg, iris.data, iris.target, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores))\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\n",
    "print(\"Variance in cross-validation score: {:.4f}\".format(np.var(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Custom CV splits\n",
    "- You can build folds manually with [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html?highlight=kfold#sklearn.model_selection.KFold) or [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)\n",
    "    - randomizable (`shuffle` parameter)\n",
    "- [LeaveOneOut](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html?highlight=leave%20one%20out#sklearn.model_selection.LeaveOneOut) does leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores KFold(n_splits=5):\n",
      "[1.    1.    0.867 0.933 0.833]\n",
      "Cross-validation scores StratifiedKFold(n_splits=5, shuffle=True):\n",
      "[0.967 0.933 0.967 0.967 0.967]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "kfold = KFold(n_splits=5)\n",
    "print(\"Cross-validation scores KFold(n_splits=5):\\n{}\".format(\n",
    "      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "print(\"Cross-validation scores StratifiedKFold(n_splits=5, shuffle=True):\\n{}\".format(\n",
    "      cross_val_score(logreg, iris.data, iris.target, cv=skfold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cv iterations:  150\n",
      "Mean accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "scores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\n",
    "print(\"Number of cv iterations: \", len(scores))\n",
    "print(\"Mean accuracy: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shuffle-split\n",
    "These shuffle the data before splitting it.\n",
    "- `ShuffleSplit` and `StratifiedShuffleSplit` (recommended for classification)\n",
    "- `train_size` and `test_size` can be absolute numbers or a percentage of the total dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:\n",
      "[0.973 0.973 0.973 0.96  0.973 0.973 0.947 0.933 0.973 0.987]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "shuffle_split = StratifiedShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\n",
    "scores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)\n",
    "print(\"Cross-validation scores:\\n{}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Grouped cross-validation\n",
    "- Add an array with group membership to `cross_val_scores` \n",
    "- Use `GroupKFold` with the number of groups as CV procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score(logreg, X, y, groups, cv=GroupKFold(n_splits=4)\n",
      "Cross-validation scores :\n",
      "[0.667 0.667 1.    0.667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "# create synthetic dataset\n",
    "X, y = make_blobs(n_samples=12, random_state=0)\n",
    "# the first three samples belong to the same group, etc.\n",
    "groups = [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]\n",
    "scores = cross_val_score(logreg, X, y, groups, cv=GroupKFold(n_splits=4))\n",
    "print(\"cross_val_score(logreg, X, y, groups, cv=GroupKFold(n_splits=4)\")\n",
    "print(\"Cross-validation scores :\\n{}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Binary classification\n",
    "- [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion%20matrix#sklearn.metrics.confusion_matrix) returns a matrix counting how many test examples are predicted correctly or 'confused' with other metrics.\n",
    "- [sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html?highlight=metrics#module-sklearn.metrics) contains implementations many of the metrics discussed in class\n",
    "    - They are all implemented so that 'higher is better'. \n",
    "- [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) computes accuracy explictly\n",
    "- [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) returns a table of binary measures, per class, and aggregated according to different aggregation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix(y_test, y_pred): \n",
      " [[48  5]\n",
      " [ 5 85]]\n",
      "accuracy_score(y_test, y_pred):  0.9300699300699301\n",
      "model.score(X_test, y_test):  0.9300699300699301\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, stratify=data.target, random_state=0)\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"confusion_matrix(y_test, y_pred): \\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"accuracy_score(y_test, y_pred): \", accuracy_score(y_test, y_pred))\n",
    "print(\"model.score(X_test, y_test): \", lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        53\n",
      "           1       0.94      0.94      0.94        90\n",
      "\n",
      "    accuracy                           0.93       143\n",
      "   macro avg       0.93      0.93      0.93       143\n",
      "weighted avg       0.93      0.93      0.93       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.dpi'] = 100 \n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explictly define the averaging function for class-level metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro average f1 score: 0.930\n",
      "Weighted average f1 score: 0.930\n",
      "Macro average f1 score: 0.925\n"
     ]
    }
   ],
   "source": [
    "pred = lr.predict(X_test)\n",
    "print(\"Micro average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"micro\")))\n",
    "print(\"Weighted average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"weighted\")))\n",
    "print(\"Macro average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilistic predictions\n",
    "To retrieve the uncertainty in the prediction, scikit-learn offers 2 functions. Often, both are available for every learner, but not always.\n",
    "\n",
    "- decision_function: returns floating point (-Inf,Inf) value for each prediction\n",
    "- predict_proba: returns probability [0,1] for each prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can also use these to compute any metric with non-standard thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold -0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.91        53\n",
      "           1       0.94      0.97      0.95        90\n",
      "\n",
      "    accuracy                           0.94       143\n",
      "   macro avg       0.94      0.93      0.93       143\n",
      "weighted avg       0.94      0.94      0.94       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Threshold -0.8\")\n",
    "y_pred_lower_threshold = lr.decision_function(X_test) > -.8\n",
    "print(classification_report(y_test, y_pred_lower_threshold))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision-Recall and ROC curves\n",
    "\n",
    "- [precision_recall_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html?highlight=precision_recall_curve) returns all precision and recall values for all possible thresholds\n",
    "- [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html?highlight=roc%20curve#sklearn.metrics.roc_curve) does the same for TPR and FPR.\n",
    "- The average precision score is returned by the `average_precision_score` measure \n",
    "- The area under the ROC curve is returned by the `roc_auc_score` measure \n",
    "    - Don't use `auc` (this uses a less accurate trapezoidal rule)\n",
    "    - Require a decision function or predict_proba.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_test, lr.decision_function(X_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision of logreg: 0.996\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "ap_pp = average_precision_score(y_test, lr.predict_proba(X_test)[:, 1])\n",
    "ap_df = average_precision_score(y_test, lr.decision_function(X_test))\n",
    "print(\"Average precision of logreg: {:.3f}\".format(ap_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for Random Forest: 0.992\n",
      "AUC for SVC: 0.992\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "rf_auc = roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1])\n",
    "svc_auc = roc_auc_score(y_test, lr.decision_function(X_test))\n",
    "print(\"AUC for Random Forest: {:.3f}\".format(rf_auc))\n",
    "print(\"AUC for SVC: {:.3f}\".format(svc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro average f1 score: 0.930\n",
      "Weighted average f1 score: 0.930\n",
      "Macro average f1 score: 0.925\n"
     ]
    }
   ],
   "source": [
    "print(\"Micro average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"micro\")))\n",
    "print(\"Weighted average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"weighted\")))\n",
    "print(\"Macro average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using evaluation metrics in model selection\n",
    "\n",
    "- You typically want to use AUC or other relevant measures in `cross_val_score` and `GridSearchCV` instead of the default accuracy.\n",
    "- scikit-learn makes this easy through the `scoring` argument\n",
    "    - But, you need to need to look the [mapping between the scorer and the metric](http://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![scorers](../images/03_scoring.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or simply look up like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scorers:\n",
      "['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted', 'max_error', 'mutual_info_score', 'neg_brier_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_gamma_deviance', 'neg_mean_poisson_deviance', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'neg_root_mean_squared_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'roc_auc_ovo', 'roc_auc_ovo_weighted', 'roc_auc_ovr', 'roc_auc_ovr_weighted', 'v_measure_score']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.scorer import SCORERS\n",
    "print(\"Available scorers:\\n{}\".format(sorted(SCORERS.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cross-validation with AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default scoring: [0.975 0.992 1.    0.994 0.981]\n",
      "Explicit accuracy scoring: [0.975 0.992 1.    0.994 0.981]\n",
      "AUC scoring: [0.997 0.999 1.    1.    0.984]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn .svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "# default scoring for classification is accuracy\n",
    "print(\"Default scoring: {}\".format(\n",
    "      cross_val_score(SVC(), digits.data, digits.target == 9)))\n",
    "# providing scoring=\"accuracy\" doesn't change the results\n",
    "explicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9, \n",
    "                                     scoring=\"accuracy\")\n",
    "print(\"Explicit accuracy scoring: {}\".format(explicit_accuracy))\n",
    "roc_auc =  cross_val_score(SVC(), digits.data, digits.target == 9,\n",
    "                           scoring=\"roc_auc\")\n",
    "print(\"AUC scoring: {}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameter tuning\n",
    "Now that we know how to evaluate models, we can improve them by tuning their hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Grid search\n",
    "- Create a parameter grid as a dictionary\n",
    "    - Keys are parameter names\n",
    "    - Values are lists of hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter grid:\n",
      "{'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "print(\"Parameter grid:\\n{}\".format(param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- `GridSearchCV`: like a classifier that uses CV to automatically optimize its hyperparameters internally\n",
    "    - Input: (untrained) model, parameter grid, CV procedure\n",
    "    - Output: optimized model on given training data\n",
    "    - Should only have access to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.svm import SVC\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        iris.data, iris.target, random_state=0)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The optimized test score and hyperparameters can easily be retrieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.97\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'gamma': 0.1}\n",
      "Best cross-validation score: 0.97\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When hyperparameters depend on other parameters, we can use lists of dictionaries to define the hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of grids:\n",
      "[{'kernel': ['rbf'], 'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}, {'kernel': ['linear'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\n"
     ]
    }
   ],
   "source": [
    "param_grid = [{'kernel': ['rbf'],\n",
    "               'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "               'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "              {'kernel': ['linear'],\n",
    "               'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\n",
    "print(\"List of grids:\\n{}\".format(param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Nested cross-validation\n",
    "\n",
    "- Nested cross-validation:\n",
    "    - Outer loop: split data in training and test sets\n",
    "    - Inner loop: run grid search, splitting the training data into train and validation sets\n",
    "- Result is a just a list of scores\n",
    "    - There will be multiple optimized models and hyperparameter settings (not returned)\n",
    "- To apply on future data, we need to train `GridSearchCV` on all data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:  [0.967 1.    0.9   0.967 1.   ]\n",
      "Mean cross-validation score:  0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),\n",
    "                         iris.data, iris.target, cv=5)\n",
    "print(\"Cross-validation scores: \", scores)\n",
    "print(\"Mean cross-validation score: \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Parallelizing cross-validation and grid-search\n",
    "- On a practical note, it is easy to parallellize CV and grid search\n",
    "- `cross_val_score` and `GridSearchCV` have a `n_jobs` parameter defining the number of cores it can use.\n",
    "    - set it to `n_jobs=-1` to use all available cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Search\n",
    "- `RandomizedSearchCV` works like `GridSearchCV`\n",
    "- Has `n_iter` parameter for the number of iterations\n",
    "- Search grid can use distributions instead of fixed lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score=nan,\n",
       "                   estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                 class_weight=None, coef0=0.0,\n",
       "                                 decision_function_shape='ovr', degree=3,\n",
       "                                 gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                 probability=False, random_state=None,\n",
       "                                 shrinking=True, tol=0.001, verbose=False),\n",
       "                   iid='deprecated', n_iter=20, n_jobs=None,\n",
       "                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x11d558828>,\n",
       "                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x11d558eb8>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon\n",
    "\n",
    "param_grid = {'C': expon(scale=100), \n",
    "              'gamma': expon(scale=.1)}\n",
    "random_search = RandomizedSearchCV(SVC(), param_distributions=param_grid,\n",
    "                                   n_iter=20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        iris.data, iris.target, random_state=0)\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building Pipelines\n",
    "* In scikit-learn, a `pipeline` combines multiple processing _steps_ in a single estimator\n",
    "* All but the last step should be transformer (have a `transform` method)\n",
    "    * The last step can be a transformer too (e.g. Scaler+PCA)\n",
    "* It has a `fit`, `predict`, and `score` method, just like any other learning algorithm\n",
    "* Pipelines are built as a list of steps, which are (name, algorithm) tuples\n",
    "    * The name can be anything you want, but can't contain `'__'`\n",
    "    * We use `'__'` to refer to the hyperparameters, e.g. `svm__C`\n",
    "* Let's build, train, and score a `MinMaxScaler` + `LinearSVC` pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "``` python\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", LinearSVC())])\n",
    "pipe.fit(X_train, y_train).score(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", LinearSVC())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                    random_state=1)\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Now with cross-validation:\n",
    "``` python\n",
    "scores = cross_val_score(pipe, cancer.data, cancer.target)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.982 0.974 0.965 0.965 0.991]\n",
      "Average cross-validation score: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(pipe, cancer.data, cancer.target)\n",
    "print(\"Cross-validation scores: {}\".format(scores))\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* We can retrieve the trained SVM by querying the right step indices\n",
    "``` python\n",
    "pipe.steps[1][1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM component: LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "print(\"SVM component: {}\".format(pipe.steps[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Or we can use the `named_steps` dictionary\n",
    "``` python\n",
    "pipe.named_steps['svm']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM component: LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM component: {}\".format(pipe.named_steps['svm']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* When you don't need specific names for specific steps, you can use `make_pipeline`\n",
    "    * Assigns names to steps automatically\n",
    "``` python\n",
    "pipe_short = make_pipeline(MinMaxScaler(), LinearSVC(C=100))\n",
    "print(\"Pipeline steps:\\n{}\".format(pipe_short.steps))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps:\n",
      "[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('linearsvc', LinearSVC(C=100, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0))]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "# abbreviated syntax\n",
    "pipe_short = make_pipeline(MinMaxScaler(), LinearSVC(C=100))\n",
    "print(\"Pipeline steps:\\n{}\".format(pipe_short.steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Visualization of a pipeline `fit` and `predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/07_pipelines.png\" alt=\"ml\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using Pipelines in Grid-searches\n",
    "* We can use the pipeline as a single estimator in `cross_val_score` or `GridSearchCV`\n",
    "* To define a grid, refer to the hyperparameters of the steps\n",
    "    * Step `svm`, parameter `C` becomes `svm__C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation accuracy: 0.97\n",
      "Test set score: 0.97\n",
      "Best parameters: {'svm__C': 10, 'svm__gamma': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe = pipeline.Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* When we request the best estimator of the grid search, we'll get the best pipeline\n",
    "``` python\n",
    "grid.best_estimator_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(memory=None,\n",
      "         steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
      "                ('svm',\n",
      "                 SVC(C=10, break_ties=False, cache_size=200, class_weight=None,\n",
      "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
      "                     gamma=1, kernel='rbf', max_iter=-1, probability=False,\n",
      "                     random_state=None, shrinking=True, tol=0.001,\n",
      "                     verbose=False))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* And we can drill down to individual components and their properties\n",
    "``` python\n",
    "grid.best_estimator_.named_steps[\"svm\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM step:\n",
      "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n",
      "    probability=False, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Get the SVM\n",
    "print(\"SVM step:\\n{}\".format(\n",
    "      grid.best_estimator_.named_steps[\"svm\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM support vector coefficients:\n",
      "[[ -1.392  -4.069  -0.435  -0.7    -5.865  -0.414  -2.814 -10.    -10.\n",
      "   -3.418  -7.908  -0.169  -4.299  -1.137  -2.214  -0.19  -10.     -7.128\n",
      "  -10.     -0.522  -3.766  -0.012  -1.159 -10.     -0.513  -0.712 -10.\n",
      "   -1.501 -10.     10.      1.995   0.909   0.919   2.897   0.399  10.\n",
      "    9.811   0.412  10.     10.     10.      5.415   0.83    2.593   1.371\n",
      "   10.      0.279   1.555   6.589   1.487  10.      1.156   0.391   2.663\n",
      "    1.277   0.651   1.841   2.395   2.504]]\n"
     ]
    }
   ],
   "source": [
    "# Get the SVM dual coefficients (support vector weights)\n",
    "print(\"SVM support vector coefficients:\\n{}\".format(\n",
    "      grid.best_estimator_.named_steps[\"svm\"].dual_coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Grid-searching preprocessing steps and model parameters\n",
    "* We can use grid search to optimize the hyperparameters of our preprocessing steps and learning algorithms at the same time\n",
    "* Consider the following pipeline:\n",
    "    - `StandardScaler`, without hyperparameters\n",
    "    - `PolynomialFeatures`, with the max. _degree_ of polynomials\n",
    "    - `Ridge` regression, with L2 regularization parameter _alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "boston = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n",
    "                                                    random_state=0)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pipe = pipeline.make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PolynomialFeatures(),\n",
    "    Ridge())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* We don't know the optimal polynomial degree or alpha value, so we use a grid search (or random search) to find the optimal values\n",
    "``` python\n",
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n",
    "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=1)\n",
    "grid.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n",
    "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "# Note: I had to use n_jobs=1. (n_jobs=-1 stalls on my machine)\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=1)\n",
    "grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Visualing the $R^2$ results as a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(grid.cv_results_['mean_test_score'].reshape(3, -1),\n",
    "            vmin=0, cmap=\"viridis\")\n",
    "plt.xlabel(\"ridge__alpha\")\n",
    "plt.ylabel(\"polynomialfeatures__degree\")\n",
    "plt.xticks(range(len(param_grid['ridge__alpha'])), param_grid['ridge__alpha'])\n",
    "plt.yticks(range(len(param_grid['polynomialfeatures__degree'])),\n",
    "           param_grid['polynomialfeatures__degree'])\n",
    "\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Here, degree-2 polynomials help (but degree-3 ones don't), and tuning the alpha parameter helps as well.\n",
    "* Not using the polynomial features leads to suboptimal results (see the results for degree 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'polynomialfeatures__degree': 2, 'ridge__alpha': 10}\n",
      "Test-set score: 0.77\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "print(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureUnions\n",
    "- Sometimes you want to apply multiple preprocessing techniques and use the _combined_ produced features\n",
    "- Simply appending the produced features is called a `FeatureJoin`\n",
    "- Example: Apply both PCA and feature selection, and run an SVM on both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined space has 3 features\n",
      "Pipeline(memory=None,\n",
      "         steps=[('features',\n",
      "                 FeatureUnion(n_jobs=None,\n",
      "                              transformer_list=[('pca',\n",
      "                                                 PCA(copy=True,\n",
      "                                                     iterated_power='auto',\n",
      "                                                     n_components=3,\n",
      "                                                     random_state=None,\n",
      "                                                     svd_solver='auto', tol=0.0,\n",
      "                                                     whiten=False)),\n",
      "                                                ('univ_select',\n",
      "                                                 SelectKBest(k=1,\n",
      "                                                             score_func=<function f_classif at 0x123dfe2f0>))],\n",
      "                              transformer_weights=None, verbose=False)),\n",
      "                ('svm',\n",
      "                 SVC(C=10, break_ties=False, cache_size=200, class_weight=None,\n",
      "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
      "                     gamma='scale', kernel='linear', max_iter=-1,\n",
      "                     probability=False, random_state=None, shrinking=True,\n",
      "                     tol=0.001, verbose=False))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# This dataset is way too high-dimensional. Better do PCA:\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Maybe some original features where good, too?\n",
    "selection = SelectKBest(k=1)\n",
    "\n",
    "# Build estimator from PCA and Univariate selection:\n",
    "\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "# Use combined features to transform dataset:\n",
    "X_features = combined_features.fit(X, y).transform(X)\n",
    "print(\"Combined space has\", X_features.shape[1], \"features\")\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Do grid search over k, n_components and C:\n",
    "\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n",
    "\n",
    "param_grid = dict(features__pca__n_components=[1, 2, 3],\n",
    "                  features__univ_select__k=[1, 2],\n",
    "                  svm__C=[0.1, 1, 10])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid)\n",
    "grid_search.fit(X, y)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColumnTransformer\n",
    "- A pipeline applies a transformer on _all_ columns\n",
    "    - If your dataset has both numeric and categorical features, you often want to apply different techniques on each\n",
    "    - You _could_ manually split up the dataset, and then feature-join the processed features (tedious)\n",
    "- `ColumnTransformer` allows you to specify on which columns a preprocessor has to be run\n",
    "    - Either by specifying the feature names, indices, or a binary mask\n",
    "- You can include multiple transformers in a ColumnTransformer\n",
    "    - In the end the results will be feature-joined\n",
    "    - Hence, the order of the features will change!\n",
    "        The features of the last transformer will be at the end\n",
    "- Each transformer can be a pipeline\n",
    "    - Handy if you need to apply multiple preprocessing steps on a set of features\n",
    "    - E.g. use a ColumnTransformer with one sub-pipeline for numerical features and one for categorical features.\n",
    "- In the end, the columntransformer can again be included as part of a pipeline\n",
    "    - E.g. to add a classfier and include the whole pipeline in a grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Handle a dataset (Titanic) with both categorical an numeric features\n",
    "- Numeric features: impute missing values and scale\n",
    "- Categorical features: Impute missing values and apply one-hot-encoding\n",
    "- Finally, run an SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.790\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load data from https://www.openml.org/d/40945\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "\n",
    "# Alternatively X and y can be obtained directly from the frame attribute:\n",
    "# X = titanic.frame.drop('survived', axis=1)\n",
    "# y = titanic.frame['survived']\n",
    "\n",
    "# We will train our classifier with the following features:\n",
    "# Numeric Features:\n",
    "# - age: float.\n",
    "# - fare: float.\n",
    "# Categorical Features:\n",
    "# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n",
    "# - sex: categories encoded as strings {'female', 'male'}.\n",
    "# - pclass: ordinal integers {1, 2, 3}.\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_features = ['age', 'fare']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can again run optimize any of the hyperparameters (preprocessing-related ones included) in a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best logistic regression from grid search: 0.798\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print((\"best logistic regression from grid search: %.3f\"\n",
    "       % grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_metadata": {
   "author": "Joaquin Vanschoren",
   "title": "Introduction"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
